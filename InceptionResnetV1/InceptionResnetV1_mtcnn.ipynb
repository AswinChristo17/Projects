{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch is running on GPU!\n",
      "CUDA Device: NVIDIA GeForce RTX 4050 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if CUDA (GPU) is available\n",
    "if torch.cuda.is_available():\n",
    "    print(\"PyTorch is running on GPU!\")\n",
    "    print(f\"CUDA Device: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"PyTorch is running on CPU.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aswin Christo\\AppData\\Roaming\\Python\\Python311\\site-packages\\facenet_pytorch\\models\\mtcnn.py:34: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(state_dict_path)\n",
      "C:\\Users\\Aswin Christo\\AppData\\Roaming\\Python\\Python311\\site-packages\\facenet_pytorch\\models\\mtcnn.py:79: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(state_dict_path)\n",
      "C:\\Users\\Aswin Christo\\AppData\\Roaming\\Python\\Python311\\site-packages\\facenet_pytorch\\models\\mtcnn.py:132: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(state_dict_path)\n",
      "C:\\Users\\Aswin Christo\\AppData\\Roaming\\Python\\Python311\\site-packages\\facenet_pytorch\\models\\inception_resnet_v1.py:329: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(cached_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset embeddings and labels have been successfully extracted!\n",
      "Accuracy: 0.4322\n",
      "Precision: 0.4322\n",
      "Recall: 1.0000\n",
      "Confusion Matrix:\n",
      "[[ 0 67]\n",
      " [ 0 51]]\n",
      "Average distance (your face): 0.0000\n",
      "Average distance (others): 0.2586\n",
      "Model saved to 'my_face_model.pkl'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
    "from PIL import Image\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, accuracy_score\n",
    "\n",
    "# Set device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Initialize MTCNN\n",
    "mtcnn = MTCNN(keep_all=True, device=device, post_process=False)\n",
    "\n",
    "# Initialize FaceNet\n",
    "facenet = InceptionResnetV1(pretrained='vggface2').eval().to(device)\n",
    "\n",
    "# Function to extract face embeddings with memory optimization\n",
    "def extract_face_embeddings(image_path, mtcnn, facenet, max_faces=10):\n",
    "    try:\n",
    "        img = Image.open(image_path).convert('RGB')  # Load image\n",
    "        img = img.resize((160, 160))  # Resize image to reduce memory usage\n",
    "        faces = mtcnn(img)  # Detect faces\n",
    "        \n",
    "        if faces is not None:\n",
    "            # Limit number of faces to process\n",
    "            faces = faces[:max_faces]\n",
    "            faces = faces.to(device)  # Move to GPU if available\n",
    "            \n",
    "            # Process faces in smaller batches\n",
    "            batch_size = 4\n",
    "            embeddings_list = []\n",
    "            \n",
    "            for i in range(0, len(faces), batch_size):\n",
    "                batch = faces[i:i+batch_size]\n",
    "                batch_embeddings = facenet(batch)\n",
    "                embeddings_list.append(batch_embeddings)\n",
    "            \n",
    "            embeddings = torch.cat(embeddings_list, dim=0)\n",
    "            torch.cuda.empty_cache()  # Clear GPU cache\n",
    "            return embeddings\n",
    "        return None\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Error processing {image_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to build a dataset of face embeddings with memory optimization\n",
    "def build_dataset(folder_path, mtcnn, facenet):\n",
    "    embeddings_list = []\n",
    "    labels = []\n",
    "\n",
    "    for label, person_name in enumerate(os.listdir(folder_path)):\n",
    "        person_folder = os.path.join(folder_path, person_name)\n",
    "        \n",
    "        # Process a limited number of images per person to reduce memory load\n",
    "        image_files = os.listdir(person_folder)[:50]  \n",
    "        \n",
    "        for img_name in image_files:\n",
    "            img_path = os.path.join(person_folder, img_name)\n",
    "            embeddings = extract_face_embeddings(img_path, mtcnn, facenet)\n",
    "            \n",
    "            if embeddings is not None:\n",
    "                embeddings_list.append(embeddings)\n",
    "                labels.extend([label] * len(embeddings))\n",
    "            \n",
    "            torch.cuda.empty_cache()  # Clear GPU cache after each image\n",
    "\n",
    "    if embeddings_list:\n",
    "        return torch.cat(embeddings_list, dim=0), np.array(labels)\n",
    "    return None, None\n",
    "\n",
    "def is_my_face(embeddings, my_face_embeddings, threshold):\n",
    "    distances = [torch.norm(embeddings - my_face_embedding).item() for my_face_embedding in my_face_embeddings]\n",
    "    min_distance = min(distances)  # Find the smallest distance\n",
    "    return min_distance < threshold, min_distance\n",
    "\n",
    "# Main script\n",
    "if __name__ == \"__main__\":\n",
    "    # Path to your dataset\n",
    "    dataset_folder = r\"D:\\DB\\face_reg\\mtcnn_mod\"\n",
    "\n",
    "    # Build the dataset\n",
    "    embeddings, labels = build_dataset(dataset_folder, mtcnn, facenet)\n",
    "\n",
    "    if embeddings is not None and labels is not None:\n",
    "        print(\"Dataset embeddings and labels have been successfully extracted!\")\n",
    "\n",
    "        # Split dataset into your face and others\n",
    "        my_face_embeddings = embeddings[labels == 0]  # Assuming label 0 is your face\n",
    "        others_embeddings = embeddings[labels == 1]  # Assuming label 1 is others\n",
    "\n",
    "        # Test the model on the dataset\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        distances = []\n",
    "\n",
    "        # Test on your face images\n",
    "        for embedding in my_face_embeddings:\n",
    "            is_me, distance = is_my_face(embedding.unsqueeze(0), my_face_embeddings, threshold=0.8)\n",
    "            y_true.append(1)  # 1 = your face\n",
    "            y_pred.append(1 if is_me else 0)\n",
    "            distances.append(distance)\n",
    "\n",
    "        # Test on others' face images\n",
    "        for embedding in others_embeddings:\n",
    "            is_me, distance = is_my_face(embedding.unsqueeze(0), my_face_embeddings, threshold=0.8)\n",
    "            y_true.append(0)  # 0 = not your face\n",
    "            y_pred.append(1 if is_me else 0)\n",
    "            distances.append(distance)\n",
    "\n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        precision = precision_score(y_true, y_pred)\n",
    "        recall = recall_score(y_true, y_pred)\n",
    "        confusion = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "        # Print metrics\n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"Precision: {precision:.4f}\")\n",
    "        print(f\"Recall: {recall:.4f}\")\n",
    "        print(\"Confusion Matrix:\")\n",
    "        print(confusion)\n",
    "\n",
    "        # Print average distance for your face and others\n",
    "        avg_distance_my_face = np.mean([d for d, true in zip(distances, y_true) if true == 1])\n",
    "        avg_distance_others = np.mean([d for d, true in zip(distances, y_true) if true == 0])\n",
    "        print(f\"Average distance (your face): {avg_distance_my_face:.4f}\")\n",
    "        print(f\"Average distance (others): {avg_distance_others:.4f}\")\n",
    "\n",
    "        # Save the embeddings and threshold to a file\n",
    "        model_data = {\n",
    "            'embeddings': my_face_embeddings,\n",
    "            'threshold': 0.8  # You can adjust this threshold\n",
    "        }\n",
    "\n",
    "        with open('my_face_model.pkl', 'wb') as f:\n",
    "            pickle.dump(model_data, f)\n",
    "\n",
    "        print(\"Model saved to 'my_face_model.pkl'.\")\n",
    "    else:\n",
    "        print(\"No embeddings were extracted. Check your input folder.\")\n",
    "\n",
    "# Rest of the script remains the same..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset embeddings and labels have been successfully extracted!\n",
      "Accuracy: 0.9231\n",
      "Precision: 0.9531\n",
      "Recall: 0.9104\n",
      "Confusion Matrix:\n",
      "[[47  3]\n",
      " [ 6 61]]\n",
      "Model saved to 'my_face_model.pkl'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
    "from PIL import Image\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, accuracy_score\n",
    "from torchvision import transforms\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Set device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Initialize MTCNN\n",
    "mtcnn = MTCNN(keep_all=True, device=device, post_process=False)\n",
    "\n",
    "# Initialize FaceNet\n",
    "facenet = InceptionResnetV1(pretrained='vggface2').eval().to(device)\n",
    "\n",
    "# Data augmentation transformations\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    transforms.Resize((160, 160)),\n",
    "])\n",
    "\n",
    "# Function to extract face embeddings with memory optimization\n",
    "def extract_face_embeddings(image_path, mtcnn, facenet, max_faces=10):\n",
    "    try:\n",
    "        img = Image.open(image_path).convert('RGB')  # Load image\n",
    "        img = data_transforms(img)  # Apply data augmentation\n",
    "        faces = mtcnn(img)  # Detect faces\n",
    "        \n",
    "        if faces is not None:\n",
    "            # Limit number of faces to process\n",
    "            faces = faces[:max_faces]\n",
    "            faces = faces.to(device)  # Move to GPU if available\n",
    "            \n",
    "            # Process faces in smaller batches\n",
    "            batch_size = 4\n",
    "            embeddings_list = []\n",
    "            \n",
    "            for i in range(0, len(faces), batch_size):\n",
    "                batch = faces[i:i+batch_size]\n",
    "                batch_embeddings = facenet(batch)\n",
    "                embeddings_list.append(batch_embeddings)\n",
    "            \n",
    "            embeddings = torch.cat(embeddings_list, dim=0)\n",
    "            torch.cuda.empty_cache()  # Clear GPU cache\n",
    "            return embeddings\n",
    "        return None\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Error processing {image_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to build a dataset of face embeddings with memory optimization\n",
    "def build_dataset(folder_path, mtcnn, facenet):\n",
    "    embeddings_list = []\n",
    "    labels = []\n",
    "\n",
    "    for label, person_name in enumerate(os.listdir(folder_path)):\n",
    "        person_folder = os.path.join(folder_path, person_name)\n",
    "        \n",
    "        # Process a limited number of images per person to reduce memory load\n",
    "        image_files = os.listdir(person_folder)[:50]  \n",
    "        \n",
    "        for img_name in image_files:\n",
    "            img_path = os.path.join(person_folder, img_name)\n",
    "            embeddings = extract_face_embeddings(img_path, mtcnn, facenet)\n",
    "            \n",
    "            if embeddings is not None:\n",
    "                embeddings_list.append(embeddings)\n",
    "                labels.extend([label] * len(embeddings))\n",
    "            \n",
    "            torch.cuda.empty_cache()  # Clear GPU cache after each image\n",
    "\n",
    "    if embeddings_list:\n",
    "        return torch.cat(embeddings_list, dim=0), np.array(labels)\n",
    "    return None, None\n",
    "\n",
    "def train_knn_classifier(embeddings, labels):\n",
    "    knn = KNeighborsClassifier(n_neighbors=5)\n",
    "    knn.fit(embeddings.detach().cpu().numpy(), labels)  # Detach tensor before converting to NumPy\n",
    "    return knn\n",
    "\n",
    "# Main script\n",
    "if __name__ == \"__main__\":\n",
    "    # Path to your dataset\n",
    "    dataset_folder = r\"D:\\DB\\face_reg\\mtcnn_mod\"\n",
    "\n",
    "    # Build the dataset\n",
    "    embeddings, labels = build_dataset(dataset_folder, mtcnn, facenet)\n",
    "\n",
    "    if embeddings is not None and labels is not None:\n",
    "        print(\"Dataset embeddings and labels have been successfully extracted!\")\n",
    "\n",
    "        # Train a k-NN classifier\n",
    "        knn_classifier = train_knn_classifier(embeddings, labels)\n",
    "\n",
    "        # Test the model on the dataset\n",
    "        y_true = labels\n",
    "        y_pred = knn_classifier.predict(embeddings.detach().cpu().numpy())  # Detach tensor before converting to NumPy\n",
    "\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        precision = precision_score(y_true, y_pred)\n",
    "        recall = recall_score(y_true, y_pred)\n",
    "        confusion = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"Precision: {precision:.4f}\")\n",
    "        print(f\"Recall: {recall:.4f}\")\n",
    "        print(\"Confusion Matrix:\")\n",
    "        print(confusion)\n",
    "\n",
    "        model_data = {\n",
    "            'embeddings': embeddings,\n",
    "            'labels': labels,\n",
    "            'knn_classifier': knn_classifier\n",
    "        }\n",
    "\n",
    "        with open('my_face_model.pkl', 'wb') as f:\n",
    "            pickle.dump(model_data, f)\n",
    "\n",
    "        print(\"Model saved to 'my_face_model.pkl'.\")\n",
    "    else:\n",
    "        print(\"No embeddings were extracted. Check your input folder.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aswin Christo\\AppData\\Roaming\\Python\\Python311\\site-packages\\facenet_pytorch\\models\\mtcnn.py:34: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(state_dict_path)\n",
      "C:\\Users\\Aswin Christo\\AppData\\Roaming\\Python\\Python311\\site-packages\\facenet_pytorch\\models\\mtcnn.py:79: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(state_dict_path)\n",
      "C:\\Users\\Aswin Christo\\AppData\\Roaming\\Python\\Python311\\site-packages\\facenet_pytorch\\models\\mtcnn.py:132: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(state_dict_path)\n",
      "C:\\Users\\Aswin Christo\\AppData\\Roaming\\Python\\Python311\\site-packages\\facenet_pytorch\\models\\inception_resnet_v1.py:329: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(cached_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No face detected in the cropped region.\n",
      "No face detected in the cropped region.\n",
      "No face detected in the cropped region.\n",
      "No face detected in the cropped region.\n",
      "No face detected in the cropped region.\n",
      "No face detected in the cropped region.\n",
      "No faces detected in the frame.\n",
      "No faces detected in the frame.\n",
      "No faces detected in the frame.\n",
      "No face detected in the cropped region.\n",
      "No face detected in the cropped region.\n",
      "No face detected in the cropped region.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
    "from PIL import Image\n",
    "from scipy.spatial.distance import euclidean\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "mtcnn = MTCNN(keep_all=True, device=device, post_process=False)\n",
    "\n",
    "facenet = InceptionResnetV1(pretrained='vggface2').eval().to(device)\n",
    "\n",
    "with open('my_face_model.pkl', 'rb') as f:\n",
    "    model_data = pickle.load(f)\n",
    "    embeddings = model_data['embeddings']\n",
    "    labels = model_data['labels']\n",
    "    knn_classifier = model_data['knn_classifier']\n",
    "\n",
    "    embeddings = [emb.detach().cpu().numpy() if torch.is_tensor(emb) else emb for emb in embeddings]\n",
    "\n",
    "def recognize_face(embedding, knn_classifier):\n",
    "    label = knn_classifier.predict(embedding.detach().cpu().numpy().reshape(1, -1))\n",
    "    return label[0]  \n",
    "\n",
    "def calculate_distance(embedding1, embedding2):\n",
    "    embedding1 = embedding1.flatten()\n",
    "    embedding2 = embedding2.flatten()\n",
    "    return euclidean(embedding1, embedding2)\n",
    "\n",
    "cap = cv2.VideoCapture(0)  # Use 0 for the default camera\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()  # Read a frame from the camera\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    pil_image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    boxes, _ = mtcnn.detect(pil_image)\n",
    "\n",
    "    if boxes is not None:\n",
    "        boxes = boxes.astype(int)\n",
    "\n",
    "        for i, box in enumerate(boxes):\n",
    "            face = pil_image.crop((box[0], box[1], box[2], box[3]))\n",
    "            face = face.resize((160, 160))  # Resize face to match FaceNet input size\n",
    "\n",
    "            face_tensor = mtcnn(face)  # Shape: [1, 3, 160, 160]\n",
    "\n",
    "            if face_tensor is not None:\n",
    "                # Move the tensor to the correct device\n",
    "                face_tensor = face_tensor.to(device)\n",
    "\n",
    "                # Remove extra batch dimension if present\n",
    "                if face_tensor.dim() == 5:  # Check if tensor has 5 dimensions\n",
    "                    face_tensor = face_tensor.squeeze(0)  # Remove extra batch dimension\n",
    "\n",
    "                # Ensure the tensor has the correct shape [batch_size, channels, height, width]\n",
    "                if face_tensor.dim() == 4:\n",
    "                    face_tensor = face_tensor.squeeze(0)  # Remove extra batch dimension\n",
    "\n",
    "                # Extract embedding for the face\n",
    "                embedding = facenet(face_tensor.unsqueeze(0))  # Add batch dimension\n",
    "\n",
    "                # Move the embedding tensor to CPU and convert to NumPy\n",
    "                embedding_np = embedding.detach().cpu().numpy()\n",
    "\n",
    "                # Recognize if the face is yours\n",
    "                label = recognize_face(embedding, knn_classifier)\n",
    "\n",
    "                # Calculate the distance between the detected face and the stored embeddings\n",
    "                distances = [calculate_distance(embedding_np, emb) for emb in embeddings]\n",
    "                avg_distance = np.mean(distances)  # Average distance to all stored embeddings\n",
    "\n",
    "                # Draw a bounding box and label\n",
    "                cv2.rectangle(frame, (box[0], box[1]), (box[2], box[3]), (0, 255, 0), 2)\n",
    "                label_text = \"You\" if label == 0 else \"Unknown\"\n",
    "                cv2.putText(frame, f\"{label_text} (Distance: {avg_distance:.2f})\", (box[0], box[1] - 10),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "            else:\n",
    "                print(\"No face detected in the cropped region.\")\n",
    "    else:\n",
    "        print(\"No faces detected in the frame.\")\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow('Real-Time Face Recognition', frame)\n",
    "\n",
    "    # Exit on 'q' key press\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the camera and close the window\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
